spring:
  config:
    import: optional:file:.env[.properties]  # âœ… Load .env file automatically
  jackson:
    serialization:
      write-dates-as-timestamps: false
  # Spring AI Configuration
  ai:
    openai:
      api-key: ${OPENAI_API_K}
      base-url: https://api.openai.com
      chat:
        model: gpt-4
        temperature: 0.7
        max-tokens: 2000
        options:
          top-p: 1.0
          frequency-penalty: 0.0
          presence-penalty: 0.0
  mail:
    host: ${SMTP_HOST:smtp.office365.com}
    port: ${SMTP_PORT:587}
    username: ${SMTP_USERNAME}
    password: ${SMTP_PASSWORD}
    properties:
      mail.smtp.auth: true
      mail.smtp.starttls.enable: true
      mail.smtp.starttls.required: true
      mail.debug: false
server:
  port: 6050

# Alert Configuration
alerts:
  enabled: ${ALERTS_ENABLED:false}

  email:
    from: ${ALERT_FROM_EMAIL:alerts@tracebuddy.com}
    fromName: ${ALERT_FROM_NAME:TraceBuddy Performance Monitor}

  cooldown:
    minutes: ${ALERT_COOLDOWN_MINUTES:30}

  schedule:
    check: "0 */15 * * * *"  # Check every 15 minutes
proxy:
  enabled: false
  host:
  port: 8080

azure:
  service-principal:
    client-id: ${AZURE_CLIENT_ID}
    client-secret: ${AZURE_CLIENT_SECRET}
    tenant-id: ${AZURE_TENANT_ID}
  monitor:
    workspace-id: ${AZURE_WORKSPACE_ID}

# GitHub configuration
github:
  owner: ${GITHUB_OWNER:kiransahoo}
  repo: ${GITHUB_REPO:TraceWeaver}
  branch: ${GITHUB_BRANCH:main}
  token: ${GITHUB_TOKE:}
  project-path: otel-agent-with-custom/my-standalone-agent/trace-test

  mappings:
    - package-name: com.test
      provider: github
      owner: kiransahoo
      repo: TraceWeaver
      branch: main
      alerts:
        enabled: true
        timeRange: "30m"  # Check last 30 minutes
        durationThresholdMs: 1000  # Query threshold
        recipients:
          default:
            - payments-team@company.com
          critical:
            - payments-oncall@company.com
          slaBreachRecipients:
            - payments-manager@company.com
        sla:
          criticalDurationMs: 5000    # Alert if avg > 5 seconds
          highDurationMs: 2000         # Warning if avg > 2 seconds
          criticalErrorRate: 0.1       # Alert if error rate > 10%
          highErrorRate: 0.05          # Warning if error rate > 5%
          percentile: 95               # Which percentile to check
          percentileThresholdMs: 3000  # P95 should be under 3 seconds
          minSampleSize: 10            # Don't alert on low volume

    - package-name: com.xx
      provider: azure-devops
      organization: xx
      project: IT Claims
      repo: xx
      branch: master
      token: ${AZURE_DEVOPS_PAT}

    - package-name: com.xx.claims
      provider: azure-devops
      organization: xx
      project: IT Claims
      repo: xx
      branch: master
      token: ${AZURE_DEVOPS_PAT}

    - package-name: com.tracebuddy
      provider: github
      owner: kiransahoo
      repo: TraceWeaver
      branch: main

# Legacy OpenAI configuration (for your existing RestTemplate-based service)
openai:
  api:
    key: ${OPENAI_API_KEY}
    url: https://api.openai.com/v1/chat/completions

# MCP Configuration with multiple source paths
mcp:
  enabled: true  # Set to true to enable MCP
  base-path: /Users/kiransahoo/Desktop/code/otel-instrumentation/otel-agent-extension/otel-agent-with-custom/my-standalone-agent/trace-test  # Default to project root
  timeout-seconds: 10
  source: github
  # Define multiple source paths relative to base-path
  source-paths:
    java: src/main/java
    resources: src/main/resources
    test: src/test/java
    docs: docs
    config: config

logging:
  level:
    com.tracebuddy: DEBUG  # Changed from com.azureanalyzer
    com.azure: INFO
    org.springframework.ai: DEBUG

management:
  endpoints:
    web:
      exposure:
        include: health,info

# TraceBuddy specific configuration (new addition for modular architecture)
tracebuddy:
  engine:
    type: azure  # Options: azure, loki, jaeger

# LLM Engine Configuration (new addition for modular LLM)
llm:
  engine:
    type: openai  # Options: openai, claude, gemini, local